# Phase 3: Light-compute Temporal worker (embedding, LLM, email, webhooks).
# Needs: Node, pnpm, python3 (for sharp native addon build fallback).
# Embedding model (nomic-embed-text-v1.5, ~500MB ONNX) is baked into the image layer.
FROM node:22-bookworm-slim

RUN apt-get update && apt-get install -y --no-install-recommends \
  openssl \
  ca-certificates \
  python3 \
  build-essential \
  && rm -rf /var/lib/apt/lists/*

RUN corepack enable && corepack prepare pnpm@latest --activate

WORKDIR /app

COPY package.json pnpm-lock.yaml ./
COPY patches ./patches

# pnpm.onlyBuiltDependencies in package.json ensures sharp's install script runs,
# compiling the native addon for the container's platform (linux-arm64 or linux-x64).
RUN pnpm install

COPY . .

# Generate Prisma client for the container's platform
RUN pnpm prisma generate

# Pre-download the embedding model so cold start is instant (~500MB ONNX model)
# The model is cached at ~/.cache/huggingface/ inside the image layer
RUN node -e " \
  async function main() { \
    const { pipeline, env } = await import('@xenova/transformers'); \
    env.backends.onnx.wasm.numThreads = 1; \
    const p = await pipeline('feature-extraction', 'nomic-ai/nomic-embed-text-v1.5'); \
    const result = await p('warm-up test', { pooling: 'mean', normalize: true, truncation: true, max_length: 512 }); \
    console.log('Model pre-downloaded. Output dims:', result.dims); \
  } \
  main().catch(e => { console.warn('Model pre-download skipped:', e.message); process.exit(0); }); \
" || true

# ONNX WASM config: single thread reduces memory pressure
ENV ONNX_WASM_NUM_THREADS=1

CMD ["pnpm", "temporal:worker:light"]
